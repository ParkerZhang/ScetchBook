# LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!

Dacheng Li * 1 Shiyi Cao * 1 Tyler Griggs * 1 Shu Liu * 1 Xiangxi Mo 1 Shishir G. Patil 1 Matei Zaharia 1 Joseph E. Gonzalez 1 Ion Stoica 1

### Abstract

Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-ofthoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on Live-CodeBench, competitive to the proprietary o1 preview model's score of 44.6% and 59.1%.

More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at [https://github.](https://github.com/NovaSky-AI/SkyThought) [com/NovaSky-AI/SkyThought](https://github.com/NovaSky-AI/SkyThought).

### 1. Introduction

Large reasoning models (LRMs) leverage long chain-ofthoughts (Long CoTs) with reflection, backtracking, and self-validation to tackle challenging reasoning tasks [(Jaech](#page-8-0) [et al.,](#page-8-0) [2024;](#page-8-0) [Guo et al.,](#page-8-1) [2025;](#page-8-1) [Team,](#page-9-0) [2024)](#page-9-0). However, the process of eliciting Long CoTs from available LLMs remains unclear, as existing methods are either closedsourced [(Jaech et al.,](#page-8-0) [2024;](#page-8-0) [Team,](#page-9-0) [2024)](#page-9-0) or expensive to replicate [(Guo et al.,](#page-8-1) [2025)](#page-8-1).

In this paper, we first show that, surprisingly, an LLM can be cheaply and easily taught to produce Long CoT responses, significantly improving its reasoning capabilities. In particular, we find that this learning process can be both dataefficient and parameter-efficient. By performing fully supervised fine-tuning (SFT) with only 17k samples generated by DeepSeek R1, the Qwen2.5-32B-Instruct model achieves performance competitive with OpenAI o1-preview across a wide range of math and coding tasks [(Team,](#page-9-0) [2024;](#page-9-0) [Yang](#page-9-1) [et al.,](#page-9-1) [2024;](#page-9-1) [Jaech et al.,](#page-8-0) [2024)](#page-8-0). In particular, it achieves 90. 8% in Math-500 (+6.0%), 56.7% in AIME 2024 (+40.0%), 85.0% in AMC 2023 (+17.5%), 60.3% in OlympiadBench (+12.7%) and 57.0% in LiveCodeBench (+8.1%) [(Jain et al.,](#page-8-2) [2024)](#page-8-2). Even further, the model can achieve o1-preview performance by updating fewer than 5% parameters with LoRA fine-tuning [(Hu et al.,](#page-8-3) [2021)](#page-8-3). We show that the model successfully learns to reflect and revise its intermediate thoughts (e.g., frequently using reasoning keywords such as "Alternatively" and "Wait, but") and adopts long, coherent CoTs to tackle challenging problems (Fig. [1)](#page-1-0).

Moreover, we identify the Long CoT *structure* as the key characteristic of distilled data for eliciting strong reasoning performance rather than the *specific contents* of individual reasoning steps within the Long CoT. To test this, we conduct two sets of controlled studies by altering either the content of individual reasoning steps or the overall logical structure. To alter content, we perturb samples by replacing numbers with random digits or deleting reasoning keywords.

<sup>*</sup>Equal contribution 1Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. Correspondence to: Ion Stoica <istoica@berkeley.edu>.

Submission and Formatting Instructions for ICML 2025

<span id="page-1-0"></span>![](./2502.07374v1/_page_1_Figure_1.jpeg)

![](./2502.07374v1/_page_1_Figure_2.jpeg)

![](./2502.07374v1/_page_1_Figure_3.jpeg)

(b) Performance of different models on five difference reasoning benchmarks.

Figure 1: Learning to reason is data- and parameter-efficient. When fine-tuned on a small amount (17k) of Long CoT samples distilled and reject-sampled from DeepSeek-R1 with either LoRA or full-parameter tuning, the model easily learns to perform reflection and backtracking by using keywords such as "However" and "Alternatively" (Top). Consequently, the fine-tuned models improve significantly across five popular math and coding benchmarks (Bottom). For fine-tuning, the base model is Qwen2.5-32B-Instruct.

Surprisingly, we find that these perturbations have little impact on the model performance: even when 50% of numbers in training samples are randomly changed, the model only observes 3.3% lower accuracy on the most challenging math benchmark, AIME 2024, as compared to training with correct samples. To alter the global reasoning structure, we separate responses into reasoning steps and randomly shuffle, insert, or delete these steps. We observe that the trained model is much more sensitive to structural perturbations that break logical coherency in the long CoT. For example, when 67% of the training samples' reasoning steps are shuffled, accuracy drops by 13.3% on AIME 2024 problems relative to training with correct samples.

In summary, our key contributions are:

- 1. We demonstrate that an LLM can learn Long CoT
reasoning in a data-efficient and parameter-efficient manner (i.e., LoRA). With fewer than 17k samples, we fine-tune the Qwen2.5-32B-Instruct model to be competitive with o1-preview.

- 2. We identify the structure of Long CoT as critical to the learning process rather than the content of individual reasoning steps. To validate this finding, we perform two groups of controlled experiments that modify either the structure or contents of samples.
- 3. We conduct comprehensive ablations across model sizes and architectures, dataset sizes, data generation models (DeepSeek R1 and QwQ-32B-Preview), and on five popular math and coding reasoning benchmarks.

### 2. Related work

Test Time Scaling for Large Language Models Scaling test-time compute has proven effective in enhancing the the reasoning capabilities of LLMs. This can be broadly categorized into two directions: single long CoT and repeatedly sampled CoT. The former trains models, such as OpenAI o1, DeepSeek R1, and Qwen QwQ, to generate individual, long CoT responses with in-context reflection and backtracking to handle complex reasoning tasks [(Guo et al.,](#page-8-1) [2025;](#page-8-1) [Jaech](#page-8-0) [et al.,](#page-8-0) [2024;](#page-8-0) [Team,](#page-9-0) [2024)](#page-9-0). Alternatively, repeated sampling methods, such as Best-of-N or search-guided generation (e.g., MCTS), improve reasoning performance by invoking multiple responses from the model, sometimes guided by search algorithms and reward models [(Snell et al.,](#page-9-2) [2024;](#page-9-2) [Brown et al.,](#page-8-4) [2024)](#page-8-4). In this paper, we focus on distilling the ability to generate individual, Long CoTs, and show it can be done in a data- and parameter-efficient manner.

Training to improve reasoning capabilities of LLMs LLM reasoning capabilities can be improved by approaches such as iterative self-improvement and reinforcement learning (RL) [(Zelikman et al.,](#page-9-3) [2022;](#page-9-3) [Lightman et al.,](#page-9-4) [2023;](#page-9-4) [Lam](#page-8-5)[bert et al.,](#page-8-5) [2024;](#page-8-5) [Yuan et al.,](#page-9-5) [2024;](#page-9-5) [Guo et al.,](#page-8-1) [2025)](#page-8-1). More recently, Tulu-3 introduces Reinforcement Learning with Verifiable Rewards (RLVR) to improve performance in tasks such as math and coding [(Hendrycks et al.,](#page-8-6) [2021c;](#page-8-6) [Jain et al.,](#page-8-2) [2024;](#page-8-2) [LI et al.,](#page-9-6) [2024)](#page-9-6). PRIME proposes a RL-based method without process labels [(Yuan et al.,](#page-9-5) [2024)](#page-9-5). The recent release of DeepSeek R1 [(Guo et al.,](#page-8-1) [2025)](#page-8-1) demonstrates that LLMs can learn to produce long CoT and improve reasoning using a pure RL-based approach. Instead of bootstrapping reasoning ability, this paper focuses on the surprising data- and parameter-efficiency of distilling reasoning abilities from an existing reasoning model to an LLM.

Distillation Distilling the outputs or logits generated by a larger or more capable model has become a standard technique to enhance model performance [(Hinton,](#page-8-7) [2015)](#page-8-7). Typically, responses generated by higher-quality models are used to perform supervised fine-tuning on smaller models [(Lambert et al.,](#page-8-5) [2024)](#page-8-5). The Vicuna model, for instance, demonstrates that ChatGPT-generated responses can be used to effectively and cheaply distill high-quality chatting capabilities [(Zheng et al.,](#page-9-7) [2023)](#page-9-7). In this paper, we show that reasoning capabilities can also be cheaply distilled. We note that concurrent work has also observed similar trends in distilling reasoning capability [(Min et al.,](#page-9-8) [2024;](#page-9-8) [Huang](#page-8-8) [et al.,](#page-8-8) [2024)](#page-8-8). Our paper differs from these recent works by demonstrating that reasoning distillation can be achieved efficiently with minimal parameter updates. We also provide an in-depth analysis of the key factors driving reasoning improvements, including the roles of the reasoning structure and content, as well as comprehensive evaluations and ablations across different data sizes and teacher models.

### <span id="page-2-4"></span>3. Simple distillation is effective

In this section, we present our distillation process and show that a small amount of *well-curated* data, along with a simple parameter-efficient fine-tuning method (e.g., LoRA), can effectively improve reasoning capabilities in a large language model.

#### 3.1. Experiments Setup

Distillation data curation. We use DeepSeek-R1 [(Guo](#page-8-1) [et al.,](#page-8-1) [2025)](#page-8-1) and QwQ-32B-Preview [(Team,](#page-9-0) [2024)](#page-9-0), two open-source models with reasoning capabilities, to generate our distillation data. We select difficult prompts from the AMC/AIME [1](#page-2-0) , Math, and Olympiad subset from the Numina-Math dataset [(LI et al.,](#page-9-6) [2024)](#page-9-6), as [Min et al.](#page-9-8) [(2024)](#page-9-8) implies that hard problems can improve performance. We also incorporate coding problems from APPS [(Hendrycks](#page-8-9) [et al.,](#page-8-9) [2021a)](#page-8-9) and TACO [(Li et al.,](#page-9-9) [2023)](#page-9-9) datasets. Specifically, we use GPT-4o-mini to classify the difficulty of the prompts according to the AoPS standard [(Achiam et al.,](#page-8-10) [2023)](#page-8-10), and select math problems of difficulty higher than Leval 3, Olympiad higher than Level 8, and all AIME/AMC problems. We verify the correctness of the traces by checking against ground truth solutions using exact matching for math problems and code execution for coding problems. In total, we curated 12k math and 5k coding problems with correct responses from QwQ to serve as our training data. For R1 samples, we directly use the public R1-17k reasoning dataset[2](#page-2-1) that is curated following a similar procedure.

Training details. We perform training using Llama-Factory [(Zheng et al.,](#page-9-10) [2024)](#page-9-10). We train the Qwen2.5-32B-Instruct using a batch size of 96, learning rate 1e-5 with a warm-up ratio of 0.1 and linear learning rate decay [(Yang](#page-9-1) [et al.,](#page-9-1) [2024)](#page-9-1), following similar hyperparameters in [(Min](#page-9-8) [et al.,](#page-9-8) [2024)](#page-9-8). We use the next token prediction loss as the training objective [(Radford,](#page-9-11) [2018)](#page-9-11). We use the same hyperparameters except a 1e-4 learning rate for LoRA fine-tuning.

Evaluation setup. We evaluate our models on five popular reasoning benchmarks for math and coding, including Math-500, OlympiadBench, AIME-2024[3](#page-2-2) , AMC23[4](#page-2-3) [(Hendrycks](#page-8-6) [et al.,](#page-8-6) [2021c;](#page-8-6) [He et al.,](#page-8-11) [2024)](#page-8-11) and LiveCodeBench [(Jain et al.,](#page-8-2) [2024)](#page-8-2). For LiveCodeBench, we report a weighted average accuracy across its easy, medium, and hard difficulty levels.

#### 3.2. Key Insights

Small amount of data is enough. In Fig. [1b,](#page-1-0) we present the performance of models fine-tuned with the 17k R1 trained

<span id="page-2-0"></span><sup>1</sup> These prompts are from previous years of competition, which do not include AIME 2024 and AMC 2023 in our evaluation suite.

<span id="page-2-1"></span><sup>2</sup> huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k.

<span id="page-2-2"></span><sup>3</sup> huggingface.co/datasets/AI-MO/aimo-validation-aime.

<span id="page-2-3"></span><sup>4</sup> huggingface.co/datasets/AI-MO/aimo-validation-amc.

samples. Both the supervised fine-tuned (SFT) and LoRA fine-tuned models learn to generate Long CoT responses and improve significantly on all benchmarks with just 17k training samples. We investigate the effect of distillation

<span id="page-3-0"></span>![](./2502.07374v1/_page_3_Figure_2.jpeg)

Figure 2: Model accuracy with different data sizes, and comparison to DeepSeek R1. The teacher model is DeepSeek R1, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. While the student model continues to benefits from more SFT data from DeepSeek R1, a small amount of data, e.g., 16k is sufficient to significantly boost the average performance by 15.2%. data size, ranging from 4k to 64k samples from R1, The results, presented in Fig. [2,](#page-3-0) shows that a small amount of data, e.g. 16k is enough to significantly improve the model performance (from average 54.2 to 69.4).

LoRA fine-tuning without performance degradation. We next investigate the extent to which distilling Long CoT reasoning is knowledge-intensive. In addition to the results using 17k R1 samples as demonstrated in Fig. [1b,](#page-1-0) we also report the results for both SFT and LoRA fine-tuning with 7k and 17k QwQ samples in Tab. [1.](#page-3-1)

Prior work [(Ghosh et al.,](#page-8-12) [2024;](#page-8-12) [Biderman et al.,](#page-8-13) [2024)](#page-8-13) suggests that LoRA fine-tuning substantially under-performs full fine-tuning for knowledge-intensive tasks, and is limited to learning response initiation and style tokens. However, our results in Fig. [1b](#page-1-0) and Tab. [1](#page-3-1) show that LoRA fine-tuned models achieve similar or even superior reasoning performance compared to full-parameter SFT across math and coding benchmarks. Additionally, we find that a model fine-tuned with LoRA using just 7k QwQ samples performs comparably to one trained on 17k QwQ-distilled samples. This demonstrates that reasoning distillation can be achieved efficiently with both minimal parameter updates and minimal data. As shown in Fig. [1a,](#page-1-0) the LoRA fine-tuned model easily learns to generate Long CoT responses with reflection and self-verification. These observations suggest that Long CoT reasoning ability may not rely on deep knowledge acquisition but rather on learning structured reasoning patterns, which can be effectively distilled in a parameter-efficient

manner. This also aligns with prior findings that methods such as Chain-of-Thought prompting elicit Short CoT reasoning primarily by shaping response structure rather than instilling deep factual knowledge [(Wei et al.,](#page-9-12) [2022;](#page-9-12) [Yao](#page-9-13) [et al.,](#page-9-13) [2023)](#page-9-13).

<span id="page-3-1"></span>Table 1: Model accuracy with SFT and LoRA (rank=64). Fine-tuning performed on Qwen2.5-32B-Instruct with QwQ samples. "Olympiad." is short for "OlympiadBench", "LCB." is short for "LiveCodeBench". We find that the learning process of Long CoT can be parameter efficient.

|                   |      |      |      | MATH500 AIME24 AMC23 Olympiad. LCB. |      |
|-------------------|------|------|------|-------------------------------------|------|
| Qwen2.5-32B-Inst. | 84.8 | 16.7 | 67.5 | 47.6                                | 48.9 |
| QwQ               | 90.4 | 33.3 | 75.0 | 58.1                                | 59.1 |
| o1-preview        | 85.5 | 44.6 | 87.5 | 59.2                                | 59.1 |
| 7k QwQ Samples    |      |      |      |                                     |      |
| SFT               | 87.8 | 33.3 | 77.5 | 57.3                                | 57.5 |
| LoRA (r=64)       | 86.6 | 40.0 | 77.5 | 57.2                                | 56.6 |
| 17k QwQ Samples   |      |      |      |                                     |      |
| SFT               | 87.8 | 33.3 | 70.0 | 56.7                                | 57.9 |
| LoRA (r=64)       | 86.6 | 33.3 | 90.0 | 56.0                                | 56.2 |

### <span id="page-3-2"></span>4. Long CoT: Structure Is The Key

Motivated by the observation that fine-tuning with a small number of samples can significantly enhance model reasoning performance, we investigate the key factors driving this improvement. Specifically, we explore the contributions of two dimensions to the learning process:

- 1. The local content within a reasoning step, including the correctness of the final answer, numbers in math derivations, and the use of reasoning keywords.
- 2. The global reasoning structure, including reflection, self-validation, and backtracking across multiple reasoning steps to form a logically coherent long CoT.

To understand their impact, we conduct two studies: (1) we perturb the content within individual reasoning steps – such as the final answer, numerical digits, and reasoning keywords[(§4.1)](#page-4-0), and (2) we modify the global reasoning structure by inserting, deleting, and shuffling reasoning steps[(§4.2)](#page-4-1). We compare the performance of models trained on perturbed samples against both the base Qwen2.5-32B-Instruct model (i.e., Original) and model trained on correct, unperturbed samples (i.e., Correct), as shown in Tab. [2.](#page-4-2) Our findings show that the learning process is highly sensitive to modifications in the global reasoning structure, but remarkably tolerant to errors in the local contents.

Experiment setup In this section, we use QwQ-32B-Preview to produce the distillation data and select a subset of 4618 correct responses as the training set (out of the

12k math data in [§3)](#page-2-4). All perturbations in this section are performed on this dataset unless otherwise stated. We train models on each separate variant of the dataset with the same hyperparameters as in [§3](#page-2-4) and report performance in Tab. [2.](#page-4-2)

#### <span id="page-4-0"></span>4.1. Wrong or Corrupted Local Content

To study the importance of local content within individual steps, we preserve the overall reasoning structure while systematically perturbing the local content in training samples with different approaches.

Wrong Answer Samples. During our training data curation process in [§3,](#page-2-4) we only include samples that yield correct final answers. To assess whether correctness of the final answer is necessary for learning reasoning patterns, we instead train the model using an equivalent number of samples (4.6k) that lead to the *wrong* answer. Surprisingly, we find that training the base model without any samples that reach a correct final answer still achieves an average accuracy of 63.1% across benchmarks, only 3.2% lower than training with entirely correct samples.

Digits Corrupted Samples. Building on the previous experiment, we next examine the role of correctness in the intermediate reasoning steps. To evaluate this, we corrupt correct samples by replacing each digit with a random number between 0 and 9. Note that this is a severe corruption that can lead to nonsensical statements such as "1+1=3". Surprisingly, even when 70% of the digits are corrupted, the model still maintains an average performance of 62%, only 4.3% below the correct sample baseline, demonstrating robustness to incorrect content. However, when all digits are corrupted, the average performance plunges to 2.7%.

Reasoning Keyword Removal. Given the prevalence of reasoning *keywords* in responses from LRMs (e.g., 'wait', 'let me think again', 'but'), one theory is that these specific phrases may invoke the reflection and back-tracking necessary to elicit strong reasoning performance. To evaluate it, we use GPT-4o-mini to identify sentences with occurrences of these reasoning keywords and randomly remove a fraction of them (e.g., 20%, 50%, 100%). Our results show that even after removing all (100%) such keywords, the model still achieves an average accuracy of 63%, which is within 3.3% of accuracy from the model trained with correct samples. This suggests that these particular keywords do not fundamentally impact the model reasoning performance.

Conclusion. We find that errors in local content – such as incorrect mathematical derivations or missing reasoning keywords – have minimal impact on overall performance.

<span id="page-4-2"></span>Table 2: Effect of trace perturbations on reasoning performance [§4.](#page-3-2) All models are trained with base Qwen2.5- 32B-Instruct. "Olympiad." is short for "OlympiadBench". In particular, we study (1) traces with modified reasoning step contents: wrong answers, corrupted digits, and removed reasoning keywords, and (2) traces with modified structure: deleted, inserted, or shuffled steps. We find that structural perturbations are far more detrimental to model accuracy than content perturbations.

|                         |      |      |      | MATH500 AIME24 AMC23 Olympiad. Avg. |      |
|-------------------------|------|------|------|-------------------------------------|------|
| Baselines               |      |      |      |                                     |      |
| Original                | 84.8 | 16.7 | 67.5 | 47.6                                | 56.7 |
| Correct                 | 89.2 | 40.0 | 77.5 | 58.5                                | 66.3 |
| Content Modifications   |      |      |      |                                     |      |
| Wrong Answers           | 88.6 | 30.0 | 77.5 | 56.1                                | 63.1 |
| Corrupted Digits        |      |      |      |                                     |      |
| 100%                    | 5.4  | 0.0  | 2.5  | 2.8                                 | 2.7  |
| 70%                     | 85.6 | 30.0 | 77.5 | 54.8                                | 62.0 |
| 50%                     | 87.6 | 36.7 | 77.5 | 55.0                                | 64.2 |
| 20%                     | 88.4 | 30.0 | 82.5 | 57.2                                | 64.5 |
| Removed keywords        |      |      |      |                                     |      |
| 100%                    | 86.6 | 33.3 | 77.5 | 54.4                                | 63.0 |
| 50%                     | 87.6 | 36.7 | 82.5 | 56.7                                | 65.9 |
| 20%                     | 87.2 | 33.3 | 72.5 | 56.1                                | 62.3 |
| Structure Modifications |      |      |      |                                     |      |
| Shuffled Steps          |      |      |      |                                     |      |
| 100%                    | 81.8 | 23.3 | 70.0 | 49.1                                | 56.1 |
| 67%                     | 82.0 | 26.7 | 72.5 | 47.6                                | 57.2 |
| 33%                     | 85.6 | 33.3 | 75.0 | 55.3                                | 62.3 |
| Deleted Steps           |      |      |      |                                     |      |
| 100%                    | 79.2 | 13.3 | 60.0 | 45.4                                | 49.5 |
| 67%                     | 84.2 | 26.7 | 55.0 | 48.1                                | 53.5 |
| 33%                     | 88.2 | 23.3 | 80.0 | 57.7                                | 62.3 |
| Inserted Steps          |      |      |      |                                     |      |
| 100%                    | 77.0 | 10.0 | 50.0 | 41.1                                | 44.5 |
| 67%                     | 81.8 | 20.0 | 60.0 | 46.0                                | 52.0 |
| 33%                     | 86.6 | 33.3 | 77.5 | 57.2                                | 63.7 |

#### <span id="page-4-1"></span>4.2. Corrupted Global Reasoning Structure

Next, we examine the importance of reasoning *structure* by performing three modifications to the reasoning traces: deletion, insertion, and shuffle. We first note that our system prompt (Appendix [C)](#page-16-0) instructs the model to generate responses with thoughts enclosed in the tags 'begin of thought' and 'end of thought' and the final solution and step-by-step explanation in 'begin of solution' and 'end of solution'. All modifications are performed on the *thoughts*, while the solution block is left unmodified.

We use Llama-3.3-70B-Instruct [(Dubey et al.,](#page-8-14) [2024)](#page-8-14) to separate each reasoning trace into distinct reasoning steps, with boundaries determined by occurrences of backtracking, selfvalidation, reflection, or other breaks from a linear sequence of thoughts. We then generated nine modified variants of the dataset by applying each modification (insertion, deletion, and shuffle – illustrated in Fig. [3)](#page-5-0) to 33%, 67%, or 100% of reasoning steps in the 4,618 correct traces. Each variant is used to train the base model, Qwen2.5-32B-Instruct, and

Submission and Formatting Instructions for ICML 2025

<span id="page-5-0"></span>![](./2502.07374v1/_page_5_Figure_1.jpeg)

Figure 3: Reasoning step modifications. To evaluate perturbations to global structure across reasoning steps, we perform three modifications: deletion, insertion, and shuffling. These modifications break logical consistency across steps and degrade model accuracy far more than changes to local content within reasoning steps.

we report the resulting performance in Tab. [2](#page-4-2) and response lengths and reasoning keyword counts in Appendix [D.](#page-16-1)

Deleted reasoning steps. As reasoning steps are increasingly deleted from the training data, model accuracy steadily declines and eventually regresses to the base model performance. Notably, retaining only the final solution and extensive step-by-step explanation (i.e., 100% deletion case) does not suffice to learn strong reasoning capabilities. This suggests that correct long CoT demonstrations alone are insufficient. Instead, examples of handling errors and dead ends with backtracking, reflection, and self-validation are important for eliciting robust reasoning.

At 67% deletion, the model imitates reasoning keywords (relative to the base model, keyword usage increases 45×, and output token increases 9×), but its accuracy does not improve accordingly. Consistent with [§4.1,](#page-4-0) this validates that merely adopting reasoning keywords and long responses is insufficient. We note, however, that as more steps are deleted, the response lengths also decrease significantly, which could contribute to reduced accuracy. We hypothesize that it is the breaking of logical consistency *between* steps that causes accuracy degradation and validate this further in the following analysis.

Inserted reasoning steps. To further validate the importance of logical structure, we replace a subset of each trace's reasoning steps with a random sample of reasoning steps from other samples in the training set that lead to correct results. Unlike deletion, this approach generally preserves the original length of the reasoning trace, ensuring that accuracy degradation is not due simply to producing fewer steps. Relative to model variants trained with deleted reasoning steps, variants trained on inserted steps generate longer responses with more reasoning keywords, yet accuracy nonetheless deteriorates to, and even below, the level of the base model.

Interestingly, each inserted step is itself coherent and originates from a correct reasoning trace in the training data. Yet these internally-coherent steps appear in sequences that lack logical consistency and often from a separate domain (e.g., a combinatorics step may be inserted into a geometry solution), leading to contradictions and disjointed reflections. For instance, the model trained with inserted reason-

ing steps frequently references earlier steps that do not exist (e.g., "Alternatively, consider a different approach" without specifying the prior approach) or enumerates edge cases in an inconsistent order (e.g., declaring a "Case 2" without "Case 1").

While the model readily produces coherent individual steps that reflect on a problem, the CoT fails to exhibit continuity *across* reasoning steps. This aligns with the observations in the deletion setting: a mere increase in reasoning steps or keywords is insufficient for robust reasoning—logical consistency across steps is a critical factor.

Shuffled reasoning steps. We next examine whether preserving the domain of each reasoning step, eliminating potential cross-domain confusion, but randomizing their order likewise impacts the model's ability to reason.

As the amount of shuffling increases, response length and reasoning keyword usage remain high, and in fact exceed the model trained on correctly ordered traces, yet accuracy declines sharply. Similar to the insertion experiments, the model imitates the syntax of per-step reasoning but loses logical consistency across steps. For instance, we find that over 92% of model responses begin with a backtracking or self-validation keyword (e.g., *"Alternatively," or "Wait"*), even though there is no preceding content to correct or reconsider. The model also references prior calculations or cases that were never actually introduced in any preceding step. Thus, while the shuffled traces still contain valid domain-specific reasoning steps, their rearrangement leads to incoherent overall solutions. In other words, domain alignment alone does not prevent logical breakdown.

Conclusion. Taken together, these findings show that providing error-free CoT demonstrations, increasing response lengths, imitating reasoning keywords and correct short CoT within individual steps, and preserving domain relevance for each step are *not* sufficient to produce effective reasoning Further, our experiments on incorrect traces [(§4.1)](#page-4-0) demonstrate that learning reasoning capability is largely robust to local inaccuracies or miscalculations. Instead, global structural consistency is essential to elicit coherent long CoTs with the reflection, revision, and validation behaviors that produce strong reasoning performance.

### 5. Ablation Study

In this section, we conduct a series of ablation studies to answer the following questions:

- 1. [(§5.1)](#page-6-0) Does fine-tuning on Long CoT data lead to degraded performance on non-reasoning tasks?
- 2. [(§5.2)](#page-6-1) How much does the Long CoT fine-tuning enhance the performance of different student models?
- 3. [(§5.3)](#page-6-2) How does Long CoT model performance compare to the Best-of-N sampling performance of the base model?
- 4. [(§5.4)](#page-7-0) How does Long CoT fine-tuning compare to Short CoT fine-tuning with the same dataset?

#### <span id="page-6-0"></span>5.1. Performance on Non-Reasoning Benchmarks

<span id="page-6-3"></span>Table 3: Distilled Model Performance on Non-Reasoning Tasks. The teacher model is QwQ-32B-Preview, and the student model is Qwen2.5-32B-Instruct. Compared to QwQ, distilled models retain most of the base model's capabilities.

|                   | MMLU | ARC-C | IEval | MGSM |
|-------------------|------|-------|-------|------|
| Qwen2.5-32B-Inst. | 74.1 | 49.4  | 78.7  | 42.3 |
| QwQ               | 71.2 | 49.7  | 42.5  | 19.1 |
| 17k R1 Samples    |      |       |       |      |
| SFT               | 73.0 | 49.0  | 77.8  | 33.7 |
| LoRA (r=256)      | 75.5 | 47.3  | 78.4  | 38.7 |
| 17k QwQ Samples   |      |       |       |      |
| SFT               | 78.4 | 49.5  | 75.8  | 33.0 |
| LoRA (r=64)       | 78.5 | 46.7  | 74.1  | 30.6 |
| 7k QwQ Samples    |      |       |       |      |
| SFT               | 79.8 | 48.6  | 70.6  | 30.1 |
| LoRA (r=64)       | 79.1 | 47.4  | 75.4  | 31.1 |

While simple distillation enhances reasoning capabilities, it is essential to ensure that these improvements do not come at the cost of catastrophic forgetting or a decline in general language understanding and instruction-following abilities, which are crucial for broader task generalization.

To assess this, we evaluate the performance of our SFT and LoRA fine-tuned models mentioned in [§3](#page-2-4) on a diverse set of benchmarks: MMLU (multi-task language understanding), ARC-C (science exam question), IEval (instructionfollowing), and MGSM (multilingual grade-school math problems) [(Hendrycks et al.,](#page-8-15) [2021b;](#page-8-15) [Clark et al.,](#page-8-16) [2018;](#page-8-16) [Mitchell et al.,](#page-9-14) [2023;](#page-9-14) [Cobbe et al.,](#page-8-17) [2021)](#page-8-17).

As shown in Tab. [3,](#page-6-3) the base instruction model (Qwen2.5- 32B-Instruct) performs well in all these tasks. The QwQ model, despite its strong reasoning capabilities, suffers significant degradation in instruction-following (i.e., 42.5% on IEval) and multilingual tasks (i.e., 19.1% on MGSM). In

contrast, fine-tuning (through both SFT and LoRA) only on a small amount of Long CoT reasoning data from R1 or QwQ allows the distilled models to retain most of the base instruction model's capabilities, avoiding the drastic performance drop seen in QwQ.

#### <span id="page-6-1"></span>5.2. Effect on Different Student Models

![](./2502.07374v1/_page_6_Figure_15.jpeg)

Figure 4: Generalization to other models. Accuracy for models of different sizes and architectures without SFT (green) and with SFT (blue). Most models show significant improvements when fine-tuned with 17k samples from R1- Preview, showing that the Long CoT fine-tuning is beneficial across models.

In this section, we examine whether Long CoT reasoning capabilities can be elicited with different student models via fine-tuning (as described in [§3)](#page-2-4). Specifically, we train with the 17k samples on Qwen2.5-7B-Math, Qwen2.5-7-Base, Qwen2.5-7B-Instruct, Llama-3.1-8B, Qwen2.5-14B-Base, Qwen2.5-14B-Instruct, Gemma2-27B-it and Qwen2.5-32B-Instruct [(Yang et al.,](#page-9-1) [2024;](#page-9-1) [Dubey et al.,](#page-8-14) [2024;](#page-8-14) [Team et al.,](#page-9-15) [2024)](#page-9-15). We find that seven out of eight models improve noticeably across multiple benchmarks, showing the effect of Long CoT as a general improvement across models. However, not all models have showed the same degree of improvements as for Qwen2.5-32B-Instuct. These findings suggest promising future directions for understanding the performance upper bound and data efficiency with various teacher and student models in the space of reasoning.

#### <span id="page-6-2"></span>5.3. Comparison to Best-of-N

As discussed in [§5.2,](#page-6-1) not all student models achieve significant performance improvements through Long CoT finetuning. We hypothesize that this variation is influenced by several factors, such as the extent to which the training data distribution differs from that of the student models and the inherent capabilities of the student models in these tasks. In this section, we compare the test-time scaling [(Ahn et al.,](#page-8-18) [2024;](#page-8-18) [Snell et al.,](#page-9-2) [2024)](#page-9-2) performance of the base model with its performance after Long CoT fine-tuning to understand the relationship between a model's ability to benefit from Long CoT fine-tuning and its intrinsic capabilities.

<span id="page-7-1"></span>![](./2502.07374v1/_page_7_Figure_1.jpeg)

Figure 5: SFT with Long CoT vs Best-of-N. Accuracy of Qwen2.5-32B-Instruct before SFT (Zero-Shot), after SFT on 17k R1 samples (Fine-tuned), and Best-of-N samples on OlympiadBench. We find that fine-tuning on Long CoT achieves performance similar to Best of 2 to 16 samples.

Specifically, we compare the performance of Long CoT finetuning against a Best-of-N sampling approach, where we generate 128 samples per prompt using an oracle verifier to select the best response. To introduce diversity, we employ a temperature of 0.5 and top-p sampling with a threshold of 0.8. The results, presented in Fig. [5,](#page-7-1) show that the Long CoT fine-tuned model performs comparably to Best-of-N sampling with 2 to 16 instances across all student models. Notably, the test-time scaling trends closely align with the improvements observed from Long CoT fine-tuning. For example, with eight parallel samples, Llama-3.1-8B achieves less than 10% accuracy on OlympiadBench, and similarly, fine-tuning with correct Long CoT traces results in only marginal improvement. A comparable trend is observed in Gemma2-27B-it and Qwen2.5-Math-7B, reinforcing the relationship between test-time sampling efficiency and the benefits of Long CoT fine-tuning.

The performance of Best-of-N sampling continues to improve beyond 128 samples, suggesting that further gains are possible. This highlights the potential for enhancing Long CoT models through context scaling or by leveraging a broader range of reasoning paths inherent to the original model, potentially unlocking even higher performance.

#### <span id="page-7-0"></span>5.4. Comparison to Short CoT Fine-tuning

In this section, we provide a direct comparison to training with short CoT. In particular, we compare results

<span id="page-7-2"></span>Table 4: Comparison of number of output tokens reasoning keywords, and the performance between training with Short or Long CoT. The original model is Qwen2.5- 32B-Instruct. Benchmarks are ordered from easy to hard, where the model trained with Long CoT learns to produce longer CoTs and uses more keywords for harder problems.

| Dataset                    | Original | Short CoT    | Long CoT     |
|----------------------------|----------|--------------|--------------|
| Avg. output tokens         |          |              |              |
| MATH500                    | 684      | 515          | 3972         |
| AMC23                      | 728      | 605          | 5037         |
| OlympiadBench              | 1275     | 948          | 8616         |
| AIME24                     | 825      | 687          | 15902        |
| Avg. keywords per response |          |              |              |
| MATH500                    | 0.00     | 0.00         | 41.75        |
| AMC23                      | 0.00     | 0.00         | 39.20        |
| OlympiadBench              | 0.01     | 0.01         | 97.20        |
| AIME24                     | 0.00     | 0.07         | 260.90       |
| Performance                |          |              |              |
| MATH500                    | 84.8     | 70.4 (-14.4) | 89.2 (+4.4)  |
| AMC23                      | 67.5     | 55.0 (-12.5) | 77.5 (+10.0) |
| OlympiadBench              | 47.6     | 36.4 (-11.2) | 58.5 (+10.9) |
| AIME24                     | 16.7     | 13.3 (-3.4)  | 40.0 (+23.3) |

training on the 4.6k samples of Long CoT generated by QwQ-32B-Preview [(§4)](#page-3-2), and the short CoT denoted in the NuminaMath-CoT dataset [(LI et al.,](#page-9-6) [2024)](#page-9-6). Tab. [4](#page-7-2) summaries the statistics. Training with Long CoT enables the model to use more reasoning keywords (full list in Appendix [B)](#page-16-2), produces longer responses to harder problems, and is the key to improved performance.

### 6. Conclusion

Large reasoning models unlock new capabilities by using a longer chain of thoughts that involves reflection and backtracking to answer challenging problems. In this paper, we show that such capability can be easily fine-tuned using a few thousand examples and with low-rank adapters. We further show that the key to the learning process is the logical structure of the samples rather than the content of individual reasoning steps. Finally, we discuss several ablations with various teacher-student models and compare them to the best-of-N approach. Together, our work deepens the understanding of what is needed to instill large language models with strong reasoning capabilities and identify potential future directions.

### Impact Statement

This paper aims to contribute to the advancement of Machine Learning. While our work may have various societal implications, we do not find any that require specific emphasis currently.

### Acknowledgement

This work is funded by the Sky Computing Lab at UC Berkeley. We extend our sincere gratitude to Sumanth Hegde, Eric Tang, and Kourosh Hakhamaneshi from AnyScale for their invaluable feedback, assistance in building the evaluation infrastructure, and and contributions to the best-of-n experiments. We are grateful for the generous compute resources support from Databricks, Lambda Labs, and Anyscale. In particular, we thank Jonathan Frankle (Databricks) and Chuan Li (Lambda Labs) for facilitating access to these resources.

### References

- <span id="page-8-10"></span>Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
- <span id="page-8-18"></span>Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: Progresses and challenges. *arXiv preprint arXiv:2402.00157*, 2024.
- <span id="page-8-13"></span>Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., et al. Lora learns less and forgets less. *arXiv preprint arXiv:2405.09673*, 2024.
- <span id="page-8-4"></span>Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: ´ Scaling inference compute with repeated sampling. *arXiv preprint arXiv:2407.21787*, 2024.
- <span id="page-8-16"></span>Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
- <span id="page-8-17"></span>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
- <span id="page-8-14"></span>Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*, 2024.
- <span id="page-8-12"></span>Ghosh, S., Evuru, C. K. R., Kumar, S., S, R., Aneja, D., Jin, Z., Duraiswami, R., and Manocha, D. A closer look at the limitations of instruction tuning. *arXiv preprint arXiv:2402.05119*, 2024.
- <span id="page-8-1"></span>Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-

centivizing reasoning capability in llms via reinforcement learning. *arXiv preprint arXiv:2501.12948*, 2025.

- <span id="page-8-11"></span>He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024.
- <span id="page-8-9"></span>Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps. *NeurIPS*, 2021a.
- <span id="page-8-15"></span>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. *International Conference on Learning Representations (ICLR)*, 2021b.
- <span id="page-8-6"></span>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. *NeurIPS*, 2021c.
- <span id="page-8-7"></span>Hinton, G. Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
- <span id="page-8-3"></span>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021.
- <span id="page-8-8"></span>Huang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E., Xia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? *arXiv preprint arXiv:2411.16489*, 2024.
- <span id="page-8-0"></span>Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. *arXiv preprint arXiv:2412.16720*, 2024.
- <span id="page-8-2"></span>Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. *arXiv preprint arXiv:2403.07974*, 2024.
- <span id="page-8-5"></span>Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. T\" ulu 3: Pushing frontiers in open language model post-training. *arXiv preprint arXiv:2411.15124*, 2024.
- Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), *Proceedings of the 17th International Conference on Machine Learning (ICML 2000)*, pp. 1207–1216, Stanford, CA, 2000. Morgan Kaufmann.
- <span id="page-9-6"></span>LI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numinamath. [\[https://huggingface.co/AI-MO/]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)) [NuminaMath-CoT\](https://github.com/]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)) [project-numina/aimo-progress-prize/]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)) [blob/main/report/numina_dataset.pdf)]([https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)), 2024.
- <span id="page-9-9"></span>Li, R., Fu, J., Zhang, B.-W., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. Taco: Topics in algorithmic code generation dataset. *arXiv preprint arXiv:2312.14852*, 2023.
- <span id="page-9-4"></span>Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's verify step by step. *arXiv preprint arXiv:2305.20050*, 2023.
- <span id="page-9-8"></span>Min, Y., Chen, Z., Jiang, J., Chen, J., Deng, J., Hu, Y., Tang, Y., Wang, J., Cheng, X., Song, H., et al. Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems. *arXiv preprint arXiv:2412.09413*, 2024.
- <span id="page-9-14"></span>Mitchell, E., Raileanu, R., Raffel, C., Levine, J., Tsvetkov, Y., and Manning, C. D. Ieval: An instruction following benchmark. *arXiv preprint arXiv:2310.07724*, 2023.
- <span id="page-9-11"></span>Radford, A. Improving language understanding by generative pre-training. 2018.
- <span id="page-9-2"></span>Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. *arXiv preprint arXiv:2408.03314*, 2024.
- <span id="page-9-15"></span>Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open ´ language models at a practical size. *arXiv preprint arXiv:2408.00118*, 2024.
- <span id="page-9-0"></span>Team, Q. Qwq: Reflect deeply on the boundaries of the unknown, 2024.
- <span id="page-9-12"></span>Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. *arXiv preprint arXiv:2201.11903*, 2022.
- <span id="page-9-1"></span>Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. *arXiv preprint arXiv:2412.15115*, 2024.
- <span id="page-9-13"></span>Yao, S. et al. Tree of thoughts: Deliberate problem solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023.
- <span id="page-9-5"></span>Yuan, L., Li, W., Chen, H., Cui, G., Ding, N., Zhang, K., Zhou, B., Liu, Z., and Peng, H. Free process rewards without process labels. *arXiv preprint arXiv:2412.01981*, 2024.
- <span id="page-9-3"></span>Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. *Advances in Neural Information Processing Systems*, 35:15476–15488, 2022.
- <span id="page-9-7"></span>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information Processing Systems*, 36: 46595–46623, 2023.
- <span id="page-9-10"></span>Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. *arXiv preprint arXiv:2403.13372*, 2024.

### A. Short and Long CoT Response Examples

#### Question

For how many integers n does the expression

$$\sqrt{\frac{\log(n^2) - (\log n)^2}{\log n - 3}}$$

represent a real number, where log denotes the base 10 logarithm?

#### Short CoT answer (1/2)

To solve the problem, we start by analyzing the given expression and the conditions under which it represents a real number. The expression is:

$$\sqrt{\frac{\log(n^2) - (\log n)^2}{\log n - 3}}$$

For the square root to represent a real number, the argument of the square root, which is the fraction inside, must be nonnegative. Additionally, since we are dealing with logarithms, n must be a positive integer. Let's break down the solution into detailed steps:

1. Condition for the Denominator The denominator of the fraction is log n − 3. For the fraction to be nonnegative, we need log n − 3 > 0. This implies that log n > 3, which translates to

$$n > 10^3 = 1000.$$

Therefore, n must be greater than 1000.

2. Simplifying the Numerator The numerator of the fraction is log(n 2 ) − (log n) 2 . Using logarithmic properties, we can rewrite log(n 2 ) as 2 log n. Thus, the numerator becomes

$$2\log n - (\log n)^2.$$

3. Analyzing the Fraction Substituting the simplified numerator into the fraction, we get:

$$\frac{2\log n - (\log n)^2}{\log n - 3}$$

For this fraction to be nonnegative, given that the denominator is positive (from step 1), the numerator 2 log n − (log n) 2 must also be nonnegative. This leads to the inequality:

$$2\log n - (\log n)^2 \ge 0.$$

4. Solving the Inequality The inequality

$$2\log n - (\log n)^2 \ge 0$$

can be rewritten as

$$(\log n)^2 - 2\log n \le 0.$$

Factoring,

```
log n(log n − 2) ≤ 0.
```
Solving this inequality, we find that

0 ≤ log n ≤ 2.

Converting back to n, this implies:

1 ≤ n ≤ 100.

5. Combining Conditions From step 1, we have n > 1000, and from step 4, we have 1 ≤ n ≤ 100. However, these two conditions are contradictory. The correct interpretation should be that n must satisfy both the condition of being greater than 1000 (from the denominator analysis) and the condition derived from the inequality, which was misinterpreted in the combination step. The correct combination should focus on the range derived from the inequality, which is 1 ≤ n ≤ 100, and disregard the incorrect application of the denominator condition in this context.

#### Short CoT answer (2/2)

6. Conclusion The integers n that satisfy the given conditions are those for which

1 ≤ n ≤ 100.

Counting these integers, we find there are

| 100 |
|-----|
|-----|

such integers.

Long CoT answer (1/4)

<|begin of thought|> So I've got this problem here:

$$\sqrt{\frac{\log(n^2) - (\log n)^2}{\log n - 3}}$$

And I need to find out for how many integer values of n this expression represents a real number. The logarithm here is base 10.

First off, since it's a square root, the expression inside the square root has to be non-negative for the whole thing to be a real number. So, that means:

$$\frac{\log(n^2) - (\log n)^2}{\log n - 3} \ge 0$$

Additionally, the argument of the logarithm has to be positive because logarithms are only defined for positive real numbers. So, n 2 > 0, which is true for all n ̸= 0, and n > 0 since n 2 > 0 is always true for n ̸= 0, but we need to ensure n > 0 because the logarithm of a negative number is undefined in real numbers.

So, n has to be a positive integer.

Now, let's simplify the expression inside the square root.

I know that log(n 2 ) = 2 log n, because of the logarithm power rule. So, substituting that in:

$$\frac{2\log n - (\log n)^2}{\log n - 3}$$

Let me denote log n = x to make it easier. So, the expression becomes:

$$\frac{2x - x^2}{x - 3}$$

I can factor the numerator:

$$2x - x^2 = x(2 - x)$$

So, the expression is:

$$\frac{x(2-x)}{x-3}$$

I need to analyze when this fraction is non-negative.

To do that, I can look at the sign of each factor:

- x = log n - 2 − x = 2 − log n - x − 3 = log n − 3 So, the expression is:

$$\frac{\log n (2 - \log n)}{\log n - 3}$$

I need to determine when this fraction is greater than or equal to zero. Let me consider the critical points where each factor is zero: 1. log n = 0 ⇒ n = 100 = 1

2. 2 − log n = 0 ⇒ log n = 2 ⇒ n = 102 = 100 3. log n − 3 = 0 ⇒ log n = 3 ⇒ n = 103 = 1000 These points divide the number line into intervals where the expression doesn't change sign. So, the intervals to consider are: - 1 < n < 100 - 100 < n < 1000

- n > 1000

Long CoT answer (2/4)

Wait, but I need to consider n as positive integers, and n ≥ 1. But actually, n has to be at least 1, but n = 1 makes log n = 0, which is a critical point. So, let's consider the intervals: - 1 < n < 100 - n = 100 - 100 < n < 1000 - n = 1000 - n > 1000 I need to test the sign of the expression in each interval. First, let's consider n = 1: log 1 = 0, so the numerator is 0, and the denominator is 0 − 3 = −3, so the fraction is 0/ − 3 = 0, which is acceptable since it's zero. Next, for 1 < n < 100: Let's pick n = 10, for example. log 10 = 1 So, x = 1 Then, 2 − x = 1 x − 3 = −2 So, 1·1 −2 = − 1 2 < 0, which is not acceptable. Wait, but earlier I thought the expression inside the square root needs to be non-negative. So, this interval doesn't work. Next interval: n = 100 log 100 = 2 Numerator: 2 · (2 − 2) = 0 Denominator: 2 − 3 = −1 So, 0/ − 1 = 0, which is acceptable. Then, 100 < n < 1000 Let's pick n = 10000.5 = 102.5 ≈ 316.2, but since n is integer, let's take n = 300 log 300 ≈ 2.477 So, x ≈ 2.477 2 − x ≈ −0.477 x − 3 ≈ −0.523 So, 2.477·(−0.477) −0.523 ≈ −1.182 −0.523 ≈ 2.26 > 0, which is acceptable. Next, n = 1000 log 1000 = 3 Denominator: 3 − 3 = 0, so the expression is undefined. Not acceptable. Finally, n > 1000 Let's pick n = 10000 log 10000 = 4 So, x = 4 2 − x = −2 x − 3 = 1 So, 4·(−2) 1 = −8 < 0, which is not acceptable. So, summarizing: - n = 1: acceptable - 1 < n < 100: not acceptable - n = 100: acceptable - 100 < n < 1000: acceptable - n = 1000: not acceptable - n > 1000: not acceptable Therefore, the acceptable ranges are: - n = 1

| Long CoT answer (3/4)                                                                                                          |
|--------------------------------------------------------------------------------------------------------------------------------|
| - n = 100<br>- 100 < n < 1000                                                                                                  |
| Wait, but n = 100 is included in 100 < n < 1000? No, n = 100 is a separate point.                                              |
| Actually, n = 100 is the upper limit of 100 < n < 1000, but since n = 100 is acceptable, I need to include it.                 |
| Wait, no, n = 100 is a separate point where the expression is zero, which is acceptable.                                       |
| So, the acceptable n are:                                                                                                      |
| - n = 1                                                                                                                        |
| - 100 ≤ n < 1000                                                                                                               |
| Wait, but earlier I thought n = 1000 is not acceptable, but n = 1000 is not included in 100 ≤ n < 1000, which is<br>correct.   |
| So, now, I need to count the integer values of n in these ranges.                                                              |
| First, n = 1: 1 value                                                                                                          |
| Second, 100 ≤ n < 1000: that's from 100 to 999, inclusive.                                                                     |
| So, 999 − 100 + 1 = 900 values.                                                                                                |
| Therefore, total integers are 1 + 900 = 901.                                                                                   |
| But wait, earlier I thought n = 100 is acceptable, and it is included in 100 ≤ n < 1000, so no need to count it<br>separately. |
| Wait, no, n = 100 is included in 100 ≤ n < 1000, which already includes 100 to 999, so total is 900 values.                    |
| Plus n = 1, so total is 901.                                                                                                   |
| But, I need to make sure that for all n in 100 ≤ n < 1000, the expression is positive.                                         |
| Wait, except for n = 100, where it's zero, which is acceptable.                                                                |
| But in the interval 100 < n < 1000, it's positive, as I tested earlier.                                                        |
| So, yes, total is 901.                                                                                                         |
| Therefore, the number of integer values of n that satisfy the condition is 901.                                                |
| **Final Answer**                                                                                                               |
| 901                                                                                                                            |

<|end of thought|>

<|begin of solution|>

To determine for how many integer values of n the expression

$$\sqrt{\frac{\log(n^2) - (\log n)^2}{\log n - 3}}$$

represents a real number, we first note that the expression inside the square root must be non-negative:

$$\frac{\log(n^2) - (\log n)^2}{\log n - 3} \ge 0$$

Additionally, since logarithms are only defined for positive real numbers, we require n > 0.

1. Simplifying the expression

Using the logarithm power rule, we have:

$$
\log(n^2) = 2\log n
$$

Thus, the expression becomes:

$$\frac{2\log n - (\log n)^2}{\log n - 3}$$

Let x = log n. Then, the expression simplifies to:

$$\frac{x(2-x)}{x-3}$$

Long CoT answer (4/4)

2. Finding critical points The critical points are where the numerator or denominator equals zero: - x = 0 ⇒ n = 1 - 2 − x = 0 ⇒ x = 2 ⇒ n = 100 - x − 3 = 0 ⇒ x = 3 ⇒ n = 1000 3. Analyzing intervals The intervals to consider are: - 1 < n < 100 - n = 100 - 100 < n < 1000 - n = 1000 - n > 1000 Testing the sign of the expression in each interval: - For n = 1: The expression equals 0, which is acceptable. - For 1 < n < 100: The expression is negative, not acceptable. - For n = 100: The expression equals 0, acceptable. - For 100 < n < 1000: The expression is positive, acceptable. - For n = 1000: The expression is undefined, not acceptable. - For n > 1000: The expression is negative, not acceptable. 4. Counting acceptable integers The acceptable ranges are: - n = 1 - 100 ≤ n < 1000 The number of integers in 100 ≤ n < 1000 is 900 (from 100 to 999 inclusive). Adding n = 1 gives a total of: 1 + 900 = 901 Thus, the number of integer values of n that satisfy the condition is: 901

<|end of solution|>

### <span id="page-16-2"></span>B. Reasoning keywords

We create a bank of reasoning keywords by manual inspection of reasoning responses from QwQ and R1. We identify these following words and phrases as frequent indicators of reflection, backtracking, and self-validation: "Alternatively", "Wait", "Just to be thorough", "Just to make sure", "Let me just double-check", "Let me try another", "Let me verify", "Let me check", "Hmm", "But", "Maybe I should consider", "Maybe I can consider".

## <span id="page-16-0"></span>C. Long CoT System Prompt

We adopt the following system prompt from Still-2 [(Min et al.,](#page-9-8) [2024)](#page-9-8):

Long CoT System Prompt

Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin of thought|> thought with steps separated with \n\n} <|end of thought|> Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin of solution|> final formatted, precise, and clear solution <|end of solution|> Now, try to solve the following question through the above guidelines:

# <span id="page-16-1"></span>D. Average response lengths and keyword counts

| Dataset                    | 0%    | 33%   | 67%  | 100%  |  |  |  |
|----------------------------|-------|-------|------|-------|--|--|--|
| Avg. output tokens         |       |       |      |       |  |  |  |
| Math                       | 3551  | 2979  | 2078 | 482   |  |  |  |
| AMC 2023                   | 4838  | 6612  | 4623 | 609   |  |  |  |
| OlympiadBench              | 7234  | 6802  | 4978 | 595   |  |  |  |
| AIME 2024                  | 13088 | 11889 | 6798 | 620   |  |  |  |
| Avg. keywords per response |       |       |      |       |  |  |  |
| Math                       | 32    | 28    | 20   | 0.017 |  |  |  |
| AMC 2023                   | 39    | 85.6  | 77.8 | 0     |  |  |  |
| OlympiadBench              | 77    | 70    | 56   | 0.009 |  |  |  |
| AIME 2024                  | 143   | 143   | 90   | 0     |  |  |  |

Table 5: Average keyword counts and output tokens for deleted steps.

Table 6: Average keyword counts and output tokens for inserted steps.

| Dataset                    | 0%    | 33%   | 67%   | 100% |  |  |  |
|----------------------------|-------|-------|-------|------|--|--|--|
| Avg. output tokens         |       |       |       |      |  |  |  |
| Math                       | 3551  | 4189  | 3900  | 5383 |  |  |  |
| AMC 2023                   | 4838  | 7089  | 5464  | 5137 |  |  |  |
| OlympiadBench              | 7234  | 7558  | 6990  | 5407 |  |  |  |
| AIME 2024                  | 13088 | 12858 | 12864 | 5304 |  |  |  |
| Avg. keywords per response |       |       |       |      |  |  |  |
| Math                       | 32    | 39    | 39    | 41   |  |  |  |
| AMC 2023                   | 39    | 98    | 44    | 35   |  |  |  |
| OlympiadBench              | 77    | 76    | 80    | 38   |  |  |  |
| AIME 2024                  | 143   | 127   | 165   | 44   |  |  |  |

Table 7: Average keyword counts and output tokens for shuffled steps.

| Dataset                    | 0%    | 33%   | 67%   | 100%  |  |  |  |
|----------------------------|-------|-------|-------|-------|--|--|--|
| Avg. output tokens         |       |       |       |       |  |  |  |
| Math                       | 3551  | 4284  | 5784  | 5613  |  |  |  |
| AMC 2023                   | 4838  | 6802  | 10198 | 8661  |  |  |  |
| OlympiadBench              | 7234  | 8942  | 12154 | 12167 |  |  |  |
| AIME 2024                  | 13088 | 13451 | 16221 | 18054 |  |  |  |
| Avg. keywords per response |       |       |       |       |  |  |  |
| Math                       | 32    | 45    | 61    | 70    |  |  |  |
| AMC 2023                   | 39    | 65    | 74    | 67    |  |  |  |
| OlympiadBench              | 77    | 111   | 166   | 137   |  |  |  |
| AIME 2024                  | 143   | 161   | 201   | 210   |  |  |  |